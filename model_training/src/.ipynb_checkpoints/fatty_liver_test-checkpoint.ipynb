{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d8b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8bba73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('project-20-at-2022-02-04-19-37-028a1e44.json', 'r') as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abed221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['stage information', 'time of test', 'liver fibrosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8114c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from data_loader import load_examples\n",
    "from models.tagging_model import Tagging\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from transformers.models.bert.modeling_bert import (\n",
    "    BertConfig,\n",
    ")\n",
    "from trainer import Trainer\n",
    "from transformers import WEIGHTS_NAME\n",
    "from evaluate import evaluate\n",
    "\n",
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "\n",
    "def argument_parser():\n",
    "    args = argparse.ArgumentParser()\n",
    "    args.add_argument(\"--data_dir\",\n",
    "                      default=\"../data/opentag/\",\n",
    "                      type=str,\n",
    "                      help=\"The input dataset name, such as conll2003\")\n",
    "    args.add_argument(\"--bert_model\", default='bert-base-uncased', type=str,\n",
    "                      help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "                           \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\")\n",
    "    args.add_argument(\"--num_train_epochs\",\n",
    "                      default=1,\n",
    "                      type=float,\n",
    "                      help=\"Total number of training epochs to perform.\")\n",
    "    args.add_argument(\"--local_rank\",\n",
    "                      default=-1,\n",
    "                      type=int,\n",
    "                      help=\"local rank of gpu\")\n",
    "    args.add_argument(\"--debug\",\n",
    "                      default=False,\n",
    "                      action='store_true',\n",
    "                      help=\"Whether debug mode or not.\")\n",
    "    args.add_argument(\"--do_train\",\n",
    "                      default=False,\n",
    "                      action='store_true',\n",
    "                      help=\"Whether to run training.\")\n",
    "    args.add_argument(\"--no_cuda\",\n",
    "                      default=False,\n",
    "                      action='store_true',\n",
    "                      help=\"Whether to use CUDA when available\")\n",
    "    args.add_argument(\"--do_eval\",\n",
    "                      default=False,\n",
    "                      action='store_true',\n",
    "                      help=\"Whether to run eval on the test set.\")\n",
    "    args.add_argument(\"--learning_rate\",\n",
    "                      default=5e-5,\n",
    "                      type=float,\n",
    "                      help=\"The initial learning rate for Adam.\")\n",
    "    args.add_argument('--gradient_accumulation_steps',\n",
    "                      type=int,\n",
    "                      default=1,\n",
    "                      help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "    args.add_argument(\"--weight_decay\",\n",
    "                      default=0.01,\n",
    "                      type=float,\n",
    "                      help=\"Weight decay if we apply some.\")\n",
    "    args.add_argument(\"--adam_eps\",\n",
    "                      default=1e-6,\n",
    "                      type=float,\n",
    "                      help=\"Adam eps parameters\")\n",
    "    args.add_argument(\"--adam_b1\",\n",
    "                      default=1e-6,\n",
    "                      type=float,\n",
    "                      help=\"Adam b1 parameters\")\n",
    "    args.add_argument(\"--adam_b2\",\n",
    "                      default=1e-6,\n",
    "                      type=float,\n",
    "                      help=\"Adam b2 parameters\")\n",
    "    args.add_argument(\"--adam_correct_bias\",\n",
    "                      default=True,\n",
    "                      type=bool,\n",
    "                      help=\"Adam parameters\")\n",
    "    args.add_argument(\"--warmup_ratio\",\n",
    "                      default=0.06,\n",
    "                      type=float,\n",
    "                      help=\"Proportion of training to perform linear learning rate warmup for.\")\n",
    "    args.add_argument(\"--lr_schedule\",\n",
    "                      default=\"warmup_linear\",\n",
    "                      type=str,\n",
    "                      help=\"Warmup schedule.\")\n",
    "    args.add_argument(\"--train_batch_size\",\n",
    "                      default=16,\n",
    "                      type=int,\n",
    "                      help=\"Total batch size for training.\")\n",
    "    args.add_argument(\"--eval_batch_size\",\n",
    "                      default=32,\n",
    "                      type=int,\n",
    "                      help=\"Total batch size for training.\")\n",
    "    args.add_argument(\"--max_grad_norm\",\n",
    "                      default=0.0,\n",
    "                      type=float,\n",
    "                      help=\"Max grad norm.\")\n",
    "    args.add_argument(\"--max_seq_length\",\n",
    "                      default=32,\n",
    "                      type=int,\n",
    "                      help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                           \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                           \"than this will be padded.\")\n",
    "    args.add_argument(\"--max_attr_length\",\n",
    "                      default=8,\n",
    "                      type=int,\n",
    "                      help=\"The maximum total attribute (key) sequence length after WordPiece tokenization. \\n\"\n",
    "                           \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                           \"than this will be padded.\")\n",
    "    args.add_argument(\"--output_dir\",\n",
    "                      default=\"../outputs\",\n",
    "                      type=str,\n",
    "                      help=\"The output directory where the model, performance results and checkpoints will be written.\")\n",
    "    args.add_argument(\"--save_steps\",\n",
    "                      default=0,\n",
    "                      type=int,\n",
    "                      help=\"Save steps.\")\n",
    "    return args.parse_args()\n",
    "\n",
    "\n",
    "def set_up_device(args):\n",
    "    if args.no_cuda:\n",
    "        device = torch.device(\"cpu\")\n",
    "        args.num_gpu = 0\n",
    "    elif args.local_rank == -1:\n",
    "        device = torch.device(\"cuda\")\n",
    "        args.num_gpu = 1\n",
    "    else:\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        torch.distributed.init_process_group(backend=\"nccl\", init_method='env://')\n",
    "        args.num_gpu = 1\n",
    "    args.device = device\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7871fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kk = data[3]\n",
    "aa = kk['annotations'][0]['result'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f4fbbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the device is: cuda\n"
     ]
    }
   ],
   "source": [
    "args = argument_parser()\n",
    "device = set_up_device(args)\n",
    "\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.bert_model)\n",
    "\n",
    "# load pre-trained BertConfig\n",
    "config = BertConfig.from_pretrained(args.bert_model)\n",
    "print(\"the device is: {}\".format(args.device))\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer, TreebankWordTokenizer\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import nltk\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcb3dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f051e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _span_tokenize(sentence):\n",
    "    return TreebankWordTokenizer().span_tokenize(sentence)\n",
    "\n",
    "data = []\n",
    "with open('../data/ehr/dataset_train.json', 'r') as fp:\n",
    "    jsonObj = json.load(fp)\n",
    "\n",
    "for line in jsonObj:\n",
    "    context = line['content']\n",
    "    attribute = line['attribute']\n",
    "    values = line['values']\n",
    "\n",
    "    context_token_span = _span_tokenize(context)\n",
    "    attribute_token_span = _span_tokenize(attribute)\n",
    "\n",
    "    if len(values) == 0:\n",
    "        context_words = [context[span[0]: span[1]] for span in context_token_span]\n",
    "        labels = ['O'] * len(context_words)\n",
    "    else:\n",
    "        context_words = []\n",
    "        labels = []\n",
    "        value_idx = -1\n",
    "        value_start = -1\n",
    "        value_end = -1\n",
    "        for span in context_token_span:\n",
    "            if span[0] > value_end and value_idx < len(values) - 1:\n",
    "                value_idx += 1\n",
    "                value_start = values[value_idx][0]\n",
    "                value_end = values[value_idx][1]\n",
    "                value = values[value_idx][2]\n",
    "                assert context[value_start: value_end] == value\n",
    "\n",
    "            if span[0] == value_start:\n",
    "                labels.append('B-a')\n",
    "            elif value_start < span[0] < span[1] <= value_end and (labels[-1] == 'B-a' or labels[-1] == 'I-a'):\n",
    "                labels.append('I-a')\n",
    "            elif value_start < span[0] and span[1] - 1 == value_end and (context[span[1] - 1] == '.' and labels[-1] == 'B-a' or labels[-1] == 'I-a'):\n",
    "                labels.append('I-a')\n",
    "            else:\n",
    "                labels.append('O')\n",
    "\n",
    "            context_words.append(context[span[0]: span[1]])\n",
    "\n",
    "    attribute_words = [attribute[span[0]: span[1]] for span in attribute_token_span]\n",
    "    assert len(labels) == len(context_words)\n",
    "    data.append({\"context\": context_words, \"labels\": labels, 'attributes': attribute_words})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84a0e7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': ['Usage',\n",
       "  'is',\n",
       "  '7',\n",
       "  'hours',\n",
       "  '46',\n",
       "  'minutes.',\n",
       "  'There',\n",
       "  'is',\n",
       "  'no',\n",
       "  'significant',\n",
       "  'leaking',\n",
       "  'and',\n",
       "  'AHI',\n",
       "  'is',\n",
       "  '0.6.',\n",
       "  'He',\n",
       "  'does',\n",
       "  'get',\n",
       "  'his',\n",
       "  'equipment',\n",
       "  'purchased',\n",
       "  'himself',\n",
       "  'because',\n",
       "  'of',\n",
       "  'his',\n",
       "  'high',\n",
       "  'deductible',\n",
       "  'insurance..'],\n",
       " 'labels': ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " 'attributes': ['excluded', 'AHI']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de249ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cancer",
   "language": "python",
   "name": "cancer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
