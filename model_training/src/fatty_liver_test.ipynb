{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91d8b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fce1a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data with stage\n",
    "\n",
    "# with open('project-20-at-2022-02-04-19-37-028a1e44.json', 'r') as fp:\n",
    "#     data = json.load(fp)\n",
    "\n",
    "\n",
    "# res = []\n",
    "# for i in data:\n",
    "#     dic = {}\n",
    "    \n",
    "#     sent = i['data']['sent']\n",
    "#     cnt = sent.count('<')\n",
    "#     delimiters = []\n",
    "#     for j in range(cnt):\n",
    "# #         delimiters.append('<date-start-'+str(j+1)+'>')\n",
    "# #         delimiters.append('<date-end-'+str(j+1)+'>')\n",
    "#         sent=sent.replace('<date-start-'+str(j+1)+'>','')\n",
    "#         sent=sent.replace('<date-end-'+str(j+1)+'>','')\n",
    "#         delimiters.append('<start-'+str(j+1)+'>')\n",
    "#         delimiters.append('<end-'+str(j+1)+'>')\n",
    "#     regexPattern = '|'.join(map(re.escape, delimiters))\n",
    "#     split_sent = re.split(regexPattern, sent)\n",
    "#     dic['content'] = ''.join(split_sent)\n",
    "#     dic['attribute'] = 'stage'\n",
    "#     join_sent = re.split(regexPattern, sent)\n",
    "#     value_list = []\n",
    "#     cur_index = 0\n",
    "#     for x in range(len(join_sent)):\n",
    "#         index_list = []\n",
    "#         if x%2==0:\n",
    "#             cur_index += len(join_sent[x])\n",
    "#         else:\n",
    "#             index_list.append(cur_index)\n",
    "#             index_list.append(cur_index+len(join_sent[x]))\n",
    "#             index_list.append(dic['content'][cur_index:cur_index+len(join_sent[x])])\n",
    "#             cur_index += len(join_sent[x])\n",
    "#         if index_list!=[]:\n",
    "#             value_list.append(index_list) \n",
    "#     dic['values'] = value_list\n",
    "#     res.append(dic)\n",
    "# with open('fatty_train_data.json', 'w') as f:\n",
    "#     json.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfaed00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with date\n",
    "\n",
    "# with open('project-20-at-2022-02-04-19-37-028a1e44.json', 'r') as fp:\n",
    "#     data = json.load(fp)\n",
    "\n",
    "\n",
    "# res = []\n",
    "# for i in data:\n",
    "#     dic = {}\n",
    "    \n",
    "#     sent = i['data']['sent']\n",
    "#     cnt = sent.count('<')\n",
    "#     delimiters = []\n",
    "#     for j in range(cnt):\n",
    "#         delimiters.append('<date-start-'+str(j+1)+'>')\n",
    "#         delimiters.append('<date-end-'+str(j+1)+'>')\n",
    "#         sent=sent.replace('<start-'+str(j+1)+'>','')\n",
    "#         sent=sent.replace('<end-'+str(j+1)+'>','')\n",
    "# #         delimiters.append('<start-'+str(j+1)+'>')\n",
    "# #         delimiters.append('<end-'+str(j+1)+'>')\n",
    "#     regexPattern = '|'.join(map(re.escape, delimiters))\n",
    "#     split_sent = re.split(regexPattern, sent)\n",
    "#     dic['content'] = ''.join(split_sent)\n",
    "#     dic['attribute'] = 'time'\n",
    "#     join_sent = re.split(regexPattern, sent)\n",
    "#     value_list = []\n",
    "#     cur_index = 0\n",
    "#     for x in range(len(join_sent)):\n",
    "#         index_list = []\n",
    "#         if x%2==0:\n",
    "#             cur_index += len(join_sent[x])\n",
    "#         else:\n",
    "#             index_list.append(cur_index)\n",
    "#             index_list.append(cur_index+len(join_sent[x]))\n",
    "#             index_list.append(dic['content'][cur_index:cur_index+len(join_sent[x])])\n",
    "#             cur_index += len(join_sent[x])\n",
    "#         if index_list!=[]:\n",
    "#             value_list.append(index_list) \n",
    "#     dic['values'] = value_list\n",
    "#     res.append(dic)\n",
    "# with open('fatty_train_data.json', 'w') as f:\n",
    "#     json.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "375804a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with all info\n",
    "\n",
    "# with open('project-20-at-2022-02-04-19-37-028a1e44.json', 'r') as fp:\n",
    "#     data = json.load(fp)\n",
    "\n",
    "\n",
    "# res = []\n",
    "# for i in data:\n",
    "#     dic = {}\n",
    "    \n",
    "#     sent = i['data']['sent']\n",
    "#     cnt = sent.count('<')\n",
    "#     delimiters = []\n",
    "#     for j in range(cnt):\n",
    "#         delimiters.append('<date-start-'+str(j+1)+'>')\n",
    "#         delimiters.append('<date-end-'+str(j+1)+'>')\n",
    "#         delimiters.append('<start-'+str(j+1)+'>')\n",
    "#         delimiters.append('<end-'+str(j+1)+'>')\n",
    "#     regexPattern = '|'.join(map(re.escape, delimiters))\n",
    "#     split_sent = re.split(regexPattern, sent)\n",
    "#     dic['content'] = ''.join(split_sent)\n",
    "#     dic['attribute'] = 'stage time'\n",
    "#     join_sent = re.split(regexPattern, sent)\n",
    "#     value_list = []\n",
    "#     cur_index = 0\n",
    "#     for x in range(len(join_sent)):\n",
    "#         index_list = []\n",
    "#         if x%2==0:\n",
    "#             cur_index += len(join_sent[x])\n",
    "#         else:\n",
    "#             index_list.append(cur_index)\n",
    "#             index_list.append(cur_index+len(join_sent[x]))\n",
    "#             index_list.append(dic['content'][cur_index:cur_index+len(join_sent[x])])\n",
    "#             cur_index += len(join_sent[x])\n",
    "#         if index_list!=[]:\n",
    "#             value_list.append(index_list) \n",
    "#     dic['values'] = value_list\n",
    "#     res.append(dic)\n",
    "# with open('fatty_train_data.json', 'w') as f:\n",
    "#     json.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abed221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['stage information', 'time of test', 'liver fibrosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8114c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from data_loader import load_examples\n",
    "from models.tagging_model import Tagging\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from transformers.models.bert.modeling_bert import (\n",
    "    BertConfig,\n",
    ")\n",
    "from trainer import Trainer\n",
    "from transformers import WEIGHTS_NAME\n",
    "from evaluate import evaluate\n",
    "\n",
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "\n",
    "def argument_parser():\n",
    "    args = argparse.ArgumentParser()\n",
    "    args.add_argument(\"--data_dir\",\n",
    "                      default=\"../data/ehr/\",\n",
    "                      type=str,\n",
    "                      help=\"The input dataset name, such as conll2003\")\n",
    "    args.add_argument(\"--bert_model\", default='emilyalsentzer/Bio_ClinicalBERT', type=str,\n",
    "                      help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "                           \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\")\n",
    "    args.add_argument(\"--num_train_epochs\",\n",
    "                      default=1,\n",
    "                      type=float,\n",
    "                      help=\"Total number of training epochs to perform.\")\n",
    "    args.add_argument(\"--local_rank\",\n",
    "                      default=-1,\n",
    "                      type=int,\n",
    "                      help=\"local rank of gpu\")\n",
    "    args.add_argument(\"--debug\",\n",
    "                      default=False,\n",
    "                      action='store_true',\n",
    "                      help=\"Whether debug mode or not.\")\n",
    "    args.add_argument(\"--do_train\",\n",
    "                      default=False,\n",
    "                      action='store_true',\n",
    "                      help=\"Whether to run training.\")\n",
    "    args.add_argument(\"--no_cuda\",\n",
    "                      default=False,\n",
    "                      action='store_true',\n",
    "                      help=\"Whether to use CUDA when available\")\n",
    "    args.add_argument(\"--do_eval\",\n",
    "                      default=False,\n",
    "                      action='store_true',\n",
    "                      help=\"Whether to run eval on the test set.\")\n",
    "    args.add_argument(\"--learning_rate\",\n",
    "                      default=5e-5,\n",
    "                      type=float,\n",
    "                      help=\"The initial learning rate for Adam.\")\n",
    "    args.add_argument('--gradient_accumulation_steps',\n",
    "                      type=int,\n",
    "                      default=1,\n",
    "                      help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "    args.add_argument(\"--weight_decay\",\n",
    "                      default=0.01,\n",
    "                      type=float,\n",
    "                      help=\"Weight decay if we apply some.\")\n",
    "    args.add_argument(\"--adam_eps\",\n",
    "                      default=1e-6,\n",
    "                      type=float,\n",
    "                      help=\"Adam eps parameters\")\n",
    "    args.add_argument(\"--adam_b1\",\n",
    "                      default=1e-6,\n",
    "                      type=float,\n",
    "                      help=\"Adam b1 parameters\")\n",
    "    args.add_argument(\"--adam_b2\",\n",
    "                      default=1e-6,\n",
    "                      type=float,\n",
    "                      help=\"Adam b2 parameters\")\n",
    "    args.add_argument(\"--adam_correct_bias\",\n",
    "                      default=True,\n",
    "                      type=bool,\n",
    "                      help=\"Adam parameters\")\n",
    "    args.add_argument(\"--warmup_ratio\",\n",
    "                      default=0.06,\n",
    "                      type=float,\n",
    "                      help=\"Proportion of training to perform linear learning rate warmup for.\")\n",
    "    args.add_argument(\"--lr_schedule\",\n",
    "                      default=\"warmup_linear\",\n",
    "                      type=str,\n",
    "                      help=\"Warmup schedule.\")\n",
    "    args.add_argument(\"--train_batch_size\",\n",
    "                      default=16,\n",
    "                      type=int,\n",
    "                      help=\"Total batch size for training.\")\n",
    "    args.add_argument(\"--eval_batch_size\",\n",
    "                      default=32,\n",
    "                      type=int,\n",
    "                      help=\"Total batch size for training.\")\n",
    "    args.add_argument(\"--max_grad_norm\",\n",
    "                      default=0.0,\n",
    "                      type=float,\n",
    "                      help=\"Max grad norm.\")\n",
    "    args.add_argument(\"--max_seq_length\",\n",
    "                      default=128,\n",
    "                      type=int,\n",
    "                      help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                           \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                           \"than this will be padded.\")\n",
    "    args.add_argument(\"--max_attr_length\",\n",
    "                      default=8,\n",
    "                      type=int,\n",
    "                      help=\"The maximum total attribute (key) sequence length after WordPiece tokenization. \\n\"\n",
    "                           \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                           \"than this will be padded.\")\n",
    "    args.add_argument(\"--output_dir\",\n",
    "                      default=\"../outputs\",\n",
    "                      type=str,\n",
    "                      help=\"The output directory where the model, performance results and checkpoints will be written.\")\n",
    "    args.add_argument(\"--save_steps\",\n",
    "                      default=0,\n",
    "                      type=int,\n",
    "                      help=\"Save steps.\")\n",
    "    return args.parse_args()\n",
    "\n",
    "\n",
    "def set_up_device(args):\n",
    "    if args.no_cuda:\n",
    "        device = torch.device(\"cpu\")\n",
    "        args.num_gpu = 0\n",
    "    elif args.local_rank == -1:\n",
    "        device = torch.device(\"cuda\")\n",
    "        args.num_gpu = 1\n",
    "    else:\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        torch.distributed.init_process_group(backend=\"nccl\", init_method='env://')\n",
    "        args.num_gpu = 1\n",
    "    args.device = device\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f4fbbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the device is: cuda\n"
     ]
    }
   ],
   "source": [
    "args = argument_parser()\n",
    "device = set_up_device(args)\n",
    "\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.bert_model)\n",
    "\n",
    "# load pre-trained BertConfig\n",
    "config = BertConfig.from_pretrained(args.bert_model)\n",
    "print(\"the device is: {}\".format(args.device))\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer, TreebankWordTokenizer\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import nltk\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "183c1aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _span_tokenize(sentence):\n",
    "    return TreebankWordTokenizer().span_tokenize(sentence)\n",
    "\n",
    "data = []\n",
    "with open('../data/ehr/fatty_test_data.json', 'r') as fp:\n",
    "    jsonObj = json.load(fp)\n",
    "\n",
    "for line in jsonObj:\n",
    "    context = line['content']\n",
    "    attribute = line['attribute']\n",
    "    values = line['values']\n",
    "\n",
    "    context_token_span = _span_tokenize(context)\n",
    "    attribute_token_span = _span_tokenize(attribute)\n",
    "\n",
    "    if len(values) == 0:\n",
    "        context_words = [context[span[0]: span[1]] for span in context_token_span]\n",
    "        labels = ['O'] * len(context_words)\n",
    "    else:\n",
    "        context_words = []\n",
    "        labels = []\n",
    "        value_idx = -1\n",
    "        value_start = -1\n",
    "        value_end = -1\n",
    "        for span in context_token_span:\n",
    "            if span[0] > value_end and value_idx < len(values) - 1:\n",
    "                value_idx += 1\n",
    "                value_start = values[value_idx][0]\n",
    "                value_end = values[value_idx][1]\n",
    "                value = values[value_idx][2]\n",
    "                assert context[value_start: value_end] == value\n",
    "\n",
    "            if span[0] == value_start:\n",
    "                labels.append('B-a')\n",
    "            elif value_start < span[0] < span[1] <= value_end and (labels[-1] == 'B-a' or labels[-1] == 'I-a'):\n",
    "                labels.append('I-a')\n",
    "            elif value_start < span[0] and span[1] - 1 == value_end and (context[span[1] - 1] == '.' and labels[-1] == 'B-a' or labels[-1] == 'I-a'):\n",
    "                labels.append('I-a')\n",
    "            else:\n",
    "                labels.append('O')\n",
    "\n",
    "            context_words.append(context[span[0]: span[1]])\n",
    "\n",
    "    attribute_words = [attribute[span[0]: span[1]] for span in attribute_token_span]\n",
    "    assert len(labels) == len(context_words)\n",
    "    data.append({\"context\": context_words, \"labels\": labels, 'attributes': attribute_words})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d7ab67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    def __init__(self, guid, context, attributes, labels):\n",
    "        self.guid = guid\n",
    "        self.context = context\n",
    "        self.attributes = attributes\n",
    "        self.labels = labels\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9cd146b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _create_examples(data, data_type):\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(data):\n",
    "        guid = \"%s-%s\" % (data_type, i)\n",
    "        context = line['context']\n",
    "        attribute = line['attributes']\n",
    "        labels = line['labels']\n",
    "        examples.append(InputExample(guid=guid, context=context, attributes=attribute, labels=labels))\n",
    "    return examples\n",
    "\n",
    "examples = _create_examples(data,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1644d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['B-a', 'I-a', 'O', '[CLS]', '[SEP]']\n",
    "{label: i for i, label in enumerate(label_list)}\n",
    "max_seq_length = args.max_seq_length\n",
    "max_attr_length = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "782e940b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 12645, 9468, 2737, 113, 1664, 1348, 2528, 14084, 1596, 188, 1566, 10024, 4638, 4163, 27659, 114, 119, 1168, 13306, 1664, 1348, 2528, 14084, 1596, 11911, 3653, 119, 172, 3161, 1197, 15342, 1548, 1104, 11911, 1443, 1112, 14375, 1116, 119, 8362, 20080, 10294, 6202, 1119, 4163, 2941, 172, 3161, 1197, 15342, 1548, 2076, 113, 177, 19515, 3463, 114, 119, 8510, 1107, 1142, 8107, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer([\n",
    "     \"Diagnosis\",\n",
    "     \"NASH\",\n",
    "     \"(\",\n",
    "     \"nonalcoholic\",\n",
    "     \"steatohepatitis\",\n",
    "     \")\",\n",
    "     \".\",\n",
    "     \"Other\",\n",
    "     \"chronic\",\n",
    "     \"nonalcoholic\",\n",
    "     \"liver\",\n",
    "     \"disease.\",\n",
    "     \"Cirrhosis\",\n",
    "     \"of\",\n",
    "     \"liver\",\n",
    "     \"without\",\n",
    "     \"ascites.\",\n",
    "     \"unspecified\",\n",
    "     \"hepatic\",\n",
    "     \"cirrhosis\",\n",
    "     \"type\",\n",
    "     \"(\",\n",
    "     \"HCC\",\n",
    "     \"Code\",\n",
    "     \")\",\n",
    "     \".\",\n",
    "     \"documented\",\n",
    "     \"in\",\n",
    "     \"this\",\n",
    "     \"encounter\"\n",
    "   ], is_split_into_words=True,\n",
    "                                     max_length=max_seq_length, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa3288e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, tokenizer, max_seq_length, max_attr_length):\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "    features = []\n",
    "\n",
    "    for (idx, example) in enumerate(examples):\n",
    "        if idx % 10000 == 0:\n",
    "            print(\"Converting examples to features: {} of {}\".format(idx, len(examples)))\n",
    "\n",
    "        context_features = tokenizer(example.context, is_split_into_words=True,\n",
    "                                     max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
    "        label_ids = []\n",
    "\n",
    "        # align label with word token\n",
    "        word_ids = context_features.word_ids(batch_index=0)\n",
    "        for i in range(len(word_ids)):\n",
    "            wid = word_ids[i]\n",
    "            iid = context_features['input_ids'][i]\n",
    "            if iid == tokenizer.cls_token_id:\n",
    "                label_ids.append(label_map['[CLS]'])\n",
    "            elif iid == tokenizer.sep_token_id:\n",
    "                label_ids.append(label_map['[SEP]'])\n",
    "            elif wid is None:\n",
    "                label_ids.append(label_map['O'])\n",
    "            else:\n",
    "                # deal with sub-words problem\n",
    "                if i > 0 and wid == word_ids[i - 1] and (label_ids[-1] == label_map['B-a'] or\n",
    "                                                         label_ids[-1] == label_map['I-a']):\n",
    "                    label_ids.append(label_map['I-a'])\n",
    "                else:\n",
    "                    try:\n",
    "                        label_ids.append(label_map[example.labels[wid]])\n",
    "                    except Exception:\n",
    "                        print(example.context)\n",
    "                        print(example.labels)\n",
    "                        print(len(example.labels), wid)\n",
    "                        exit()\n",
    "\n",
    "        context_features['label_ids'] = label_ids\n",
    "        attribute_features = tokenizer(example.attributes, is_split_into_words=True,\n",
    "                                       max_length=max_attr_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "        assert len(context_features['input_ids']) == max_seq_length\n",
    "        assert len(context_features['attention_mask']) == max_seq_length\n",
    "        assert len(context_features['label_ids']) == max_seq_length\n",
    "        assert len(context_features['token_type_ids']) == max_seq_length\n",
    "        assert len(attribute_features['input_ids']) == max_attr_length\n",
    "        assert len(attribute_features['attention_mask']) == max_attr_length\n",
    "        assert len(attribute_features['token_type_ids']) == max_attr_length\n",
    "\n",
    "        features.append(InputFeatures(context_input_ids=context_features['input_ids'],\n",
    "                                      context_input_mask=context_features['attention_mask'],\n",
    "                                      context_input_len=max_seq_length,\n",
    "                                      context_type_ids=context_features['token_type_ids'],\n",
    "                                      attribute_input_ids=attribute_features['input_ids'],\n",
    "                                      attribute_input_mask=attribute_features['attention_mask'],\n",
    "                                      attribute_input_len=max_attr_length,\n",
    "                                      attribute_type_ids=attribute_features['token_type_ids'],\n",
    "                                      label_ids=context_features['label_ids']\n",
    "                                      ))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52278c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    def __init__(self, context_input_ids, context_input_mask, context_input_len,\n",
    "                 attribute_input_ids, attribute_input_mask, attribute_input_len,\n",
    "                 context_type_ids, attribute_type_ids, label_ids):\n",
    "        self.context_input_ids = context_input_ids\n",
    "        self.context_input_mask = context_input_mask\n",
    "        self.context_input_len = context_input_len\n",
    "        self.context_type_ids = context_type_ids\n",
    "        self.attribute_input_ids = attribute_input_ids\n",
    "        self.attribute_input_mask = attribute_input_mask\n",
    "        self.attribute_input_len = attribute_input_len\n",
    "        self.attribute_type_ids = attribute_type_ids\n",
    "        self.label_ids = label_ids\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5381f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting examples to features: 0 of 86\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features = convert_examples_to_features(examples, label_list, tokenizer, max_seq_length, max_attr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf8047bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "def collate_fn(batch):\n",
    "    def convert_to_tensor(key):\n",
    "        if isinstance(key, str):\n",
    "            tensors = [torch.tensor(getattr(o[1], key), dtype=torch.long) for o in batch]\n",
    "        else:\n",
    "            tensors = [torch.tensor(o, dtype=torch.long) for o in key]\n",
    "\n",
    "        return torch.stack(tensors)\n",
    "\n",
    "    ret = dict(context_input_ids=convert_to_tensor('context_input_ids'),\n",
    "               context_input_mask=convert_to_tensor('context_input_mask'),\n",
    "               context_type_ids=convert_to_tensor('context_type_ids'),\n",
    "               context_input_len=convert_to_tensor('context_input_len'),\n",
    "               attribute_input_ids=convert_to_tensor('attribute_input_ids'),\n",
    "               attribute_input_mask=convert_to_tensor('attribute_input_mask'),\n",
    "               attribute_type_ids=convert_to_tensor('attribute_type_ids'),\n",
    "               attribute_input_len=convert_to_tensor('attribute_input_len'),\n",
    "               label_ids=convert_to_tensor('label_ids'))\n",
    "    return ret\n",
    "\n",
    "sampler = RandomSampler(features) if args.local_rank == -1 else DistributedSampler(features)\n",
    "dataloader = DataLoader(list(enumerate(features)), batch_size=args.train_batch_size, sampler=sampler,\n",
    "                        collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e47ef089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.layers import Attention\n",
    "from models.crf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "823735df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import LayerNorm\n",
    "from transformers.models.bert.modeling_bert import (\n",
    "    BertModel,\n",
    "    BertConfig,\n",
    "    BertPreTrainedModel,\n",
    ")\n",
    "# from .models import Attention\n",
    "# from .crf import CRF\n",
    "\n",
    "class Tagging(BertPreTrainedModel):\n",
    "    def __init__(self, config, label_list, device):\n",
    "        super(Tagging, self).__init__(config)\n",
    "\n",
    "        # get bert model\n",
    "        self.bert = BertModel(config)\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "\n",
    "        self.context_lstm = nn.LSTM(input_size=self.config.hidden_size,\n",
    "                                    hidden_size=self.config.hidden_size // 2,\n",
    "                                    batch_first=True,\n",
    "                                    bidirectional=True)\n",
    "        self.attribute_lstm = nn.LSTM(input_size=self.config.hidden_size,\n",
    "                                      hidden_size=self.config.hidden_size // 2,\n",
    "                                      batch_first=True,\n",
    "                                      bidirectional=True)\n",
    "\n",
    "        self.ln = LayerNorm(self.config.hidden_size * 2)\n",
    "        self.attention = Attention()\n",
    "        self.classifier = nn.Linear(self.config.hidden_size * 2, len(label_list))\n",
    "        label2id = {k: i for i, k in enumerate(label_list)}\n",
    "        # self.crf = CRF(num_tags=len(label_list), tag2id=label2id, batch_first=True)\n",
    "        self.crf = CRF(tagset_size=len(label_list), tag_dictionary=label2id, device=device, is_bert=True)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, context_input_ids=None, context_input_mask=None, context_type_ids=None,\n",
    "                context_input_len=None, attribute_input_ids=None, attribute_input_mask=None,\n",
    "                attribute_type_ids=None, attribute_input_len=None, label_ids=None):\n",
    "\n",
    "        bert_output_context = self.bert(context_input_ids, context_type_ids, context_input_mask)\n",
    "        bert_output_attribute = self.bert(attribute_input_ids, attribute_type_ids, attribute_input_mask)\n",
    "\n",
    "        # passing bert to lstm\n",
    "        context_output, _ = self.context_lstm(bert_output_context[0])\n",
    "        _, attribute_hidden = self.attribute_lstm(bert_output_attribute[0])\n",
    "\n",
    "        # using last hidden state of attribute lstm\n",
    "        attribute_output = torch.cat([attribute_hidden[0][-2], attribute_hidden[0][-1]], dim=-1)\n",
    "\n",
    "        # get attention output\n",
    "        attention_output = self.attention(context_output, attribute_output)\n",
    "        outputs = torch.cat([context_output, attention_output], dim=-1)\n",
    "\n",
    "        outputs = self.ln(outputs)\n",
    "        outputs = self.dropout(outputs)\n",
    "        logits = self.classifier(outputs)\n",
    "        loss = None\n",
    "        if label_ids is not None:\n",
    "            # loss = self.crf.calculate_loss(emissions=logits, tags=label_ids, mask=context_input_mask)\n",
    "            loss = self.crf.calculate_loss(logits, tag_list=label_ids, lengths=context_input_len)\n",
    "        return {'loss': loss, 'logits': logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aca760ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d7b7f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing Tagging: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing Tagging from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Tagging from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Tagging were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['attribute_lstm.weight_ih_l0_reverse', 'context_lstm.weight_hh_l0', 'classifier.weight', 'classifier.bias', 'ln.bias', 'context_lstm.bias_hh_l0_reverse', 'context_lstm.bias_ih_l0', 'context_lstm.weight_ih_l0', 'context_lstm.weight_hh_l0_reverse', 'attribute_lstm.bias_hh_l0_reverse', 'attribute_lstm.bias_ih_l0_reverse', 'attribute_lstm.weight_ih_l0', 'attribute_lstm.weight_hh_l0', 'context_lstm.bias_hh_l0', 'context_lstm.weight_ih_l0_reverse', 'context_lstm.bias_ih_l0_reverse', 'attribute_lstm.bias_hh_l0', 'crf.transitions', 'attribute_lstm.bias_ih_l0', 'attribute_lstm.weight_hh_l0_reverse', 'ln.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tagging(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (context_lstm): LSTM(768, 384, batch_first=True, bidirectional=True)\n",
       "  (attribute_lstm): LSTM(768, 384, batch_first=True, bidirectional=True)\n",
       "  (ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "  (attention): Attention()\n",
       "  (classifier): Linear(in_features=1536, out_features=5, bias=True)\n",
       "  (crf): CRF()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eval\n",
    "torch.cuda.empty_cache()\n",
    "model = Tagging.from_pretrained(args.bert_model, config=config, label_list=labels_list, device=args.device)\n",
    "model.load_state_dict(torch.load(os.path.join(args.output_dir, WEIGHTS_NAME), map_location=\"cpu\"))\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c642f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    kk = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43054229",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {k: v.to(args.device) for k, v in kk.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93923ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nb_eval_steps = 0\n",
    "from metrics import SeqEntityScore\n",
    "args.id2label = {i: label for i, label in enumerate(labels_list)}\n",
    "args.label2id = {label: i for i, label in enumerate(labels_list)}\n",
    "metric = SeqEntityScore(args.id2label, markup='bio')\n",
    "\n",
    "eval_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs['logits']\n",
    "    eval_loss += outputs['loss'].item()\n",
    "    pred_labels, _ = model.crf.obtain_labels(logits, args.id2label, inputs['context_input_len'])\n",
    "    #pred_labels, _  = model.crf.obtain_labels(logits, args.id2label, inputs['context_input_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d583622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels, ff = model.crf.obtain_labels(logits, args.id2label, inputs['context_input_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d757b461",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_label_ids = inputs['label_ids'].cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c386ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_input_ids = inputs['context_input_ids']\n",
    "attribute_input_ids = inputs['attribute_input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "972acf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(seq, id2label, markup='bios'):\n",
    "    '''\n",
    "    :param seq:\n",
    "    :param id2label:\n",
    "    :param markup:\n",
    "    :return:\n",
    "    '''\n",
    "    assert markup in ['bio', 'bios']\n",
    "    if markup == 'bio':\n",
    "        return get_entity_bio(seq, id2label)\n",
    "    else:\n",
    "        return get_entity_bios(seq, id2label)\n",
    "def get_entity_bio(seq, id2label):\n",
    "    \"\"\"Gets entities from sequence.\n",
    "    note: BIO\n",
    "    Args:\n",
    "        seq (list): sequence of labels.\n",
    "    Returns:\n",
    "        list: list of (chunk_type, chunk_start, chunk_end).\n",
    "    Example:\n",
    "        seq = ['B-PER', 'I-PER', 'O', 'B-LOC']\n",
    "        get_entity_bio(seq)\n",
    "        #output\n",
    "        [['PER', 0,1], ['LOC', 3, 3]]\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    chunk = [-1, -1, -1]\n",
    "    for indx, tag in enumerate(seq):\n",
    "        if not isinstance(tag, str):\n",
    "            tag = id2label[tag]\n",
    "\n",
    "        if tag == '[SEP]' or tag == '[CLS]':\n",
    "            continue\n",
    "\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if chunk[2] != -1:\n",
    "                chunks.append(chunk)\n",
    "            chunk = [-1, -1, -1]\n",
    "            chunk[1] = indx\n",
    "            chunk[0] = tag.split('-')[1]\n",
    "            chunk[2] = indx\n",
    "            if indx == len(seq) - 1:\n",
    "                chunks.append(chunk)\n",
    "        elif tag.startswith('I-') and chunk[1] != -1:\n",
    "            _type = tag.split('-')[1]\n",
    "            if _type == chunk[0]:\n",
    "                chunk[2] = indx\n",
    "\n",
    "            if indx == len(seq) - 1:\n",
    "                chunks.append(chunk)\n",
    "        else:\n",
    "            if chunk[2] != -1:\n",
    "                chunks.append(chunk)\n",
    "            chunk = [-1, -1, -1]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad03022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "idx = 0\n",
    "all_predictions = defaultdict(dict)\n",
    "all_labels = defaultdict(dict)\n",
    "all_context_input_ids = defaultdict(dict)\n",
    "all_attribute_input_ids = defaultdict(dict)\n",
    "for i in range(len(pred_labels)):\n",
    "    all_predictions[idx] = [h for h in get_entities(pred_labels[i], args.id2label, 'bio') \n",
    "                            if h[1]!=0 and h[2]<len([k for k in inputs['context_input_mask'][i] if k==1])-1]\n",
    "    all_labels[idx] = get_entities(true_label_ids[i], args.id2label, 'bio')\n",
    "    all_context_input_ids[idx] = context_input_ids[i]\n",
    "    all_attribute_input_ids[idx] = attribute_input_ids[i]\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a75e432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_eval_steps += 1\n",
    "for i, labels in enumerate(true_label_ids):\n",
    "    true_label_path = []\n",
    "    pred_label_path = []\n",
    "    for j, m in enumerate(labels):\n",
    "        if j == 0:\n",
    "            continue\n",
    "        elif true_label_ids[i][j] == args.label2id['[SEP]']:\n",
    "            metric.update(pred_paths=[pred_label_path], label_paths=[true_label_path])\n",
    "            break\n",
    "        else:\n",
    "            true_label_path.append(args.id2label[true_label_ids[i][j]])\n",
    "            pred_label_path.append(pred_labels[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04a94648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_bert_padding(text):\n",
    "    m = re.search(r\"\\[CLS\\](.*)\\[SEP\\]\", text)\n",
    "    text = text[m.start(): m.end()][6:-6]\n",
    "    return text\n",
    "\n",
    "prediction_results = list()\n",
    "\n",
    "for i in range(idx):\n",
    "    predictions = all_predictions[i]\n",
    "    labels = all_labels[i]\n",
    "    context_input_ids = all_context_input_ids[i]\n",
    "    attribute_input_ids = all_attribute_input_ids[i]\n",
    "\n",
    "    context = tokenizer.convert_ids_to_tokens(context_input_ids)\n",
    "    context = tokenizer.convert_tokens_to_string(context)\n",
    "\n",
    "    attribute = tokenizer.convert_ids_to_tokens(attribute_input_ids)\n",
    "    attribute = tokenizer.convert_tokens_to_string(attribute)\n",
    "\n",
    "    true_label_tokens = [tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(context_input_ids[label[1]: label[2] + 1])) for label in labels]\n",
    "    pred_label_tokens = [tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(context_input_ids[pred[1]: pred[2] + 1])) for pred in predictions]\n",
    "\n",
    "    context = extract_text_from_bert_padding(context)\n",
    "    attribute = extract_text_from_bert_padding(attribute)\n",
    "    prediction_results.append({'context': context, 'attribute': attribute,\n",
    "                               'true value': ', '.join(true_label_tokens),\n",
    "                               'predict value': ', '.join(pred_label_tokens)})\n",
    "    if i==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1b44e77d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'context': 'diagnosis cirrhosis of liver without ascites. unspecified hepatic cirrhosis type ( hcc code ) - primary. nash ( nonalcoholic steatohepatitis )',\n",
       "  'attribute': 'stage',\n",
       "  'true value': 'cirrhosis',\n",
       "  'predict value': 'cirrhosis'},\n",
       " {'context': '( k75. 81 ) nonalcoholic steatohepatitis. ( e66. 9 ) obesity, bmi 35 - 39. 9, adult. mr. ryan f ingersoll is a 32 yrs male who presents for evaluation and management of vertically - acquired chronic hepatitis b and nafld / nash.. he has suffered no complications of chronic liver disease, such as ascites, variceal bleeding, hepatic encephalopathy, spontaneous bacterial peritonitis, or hepatocellular carcinoma, and he has',\n",
       "  'attribute': 'stage',\n",
       "  'true value': '',\n",
       "  'predict value': ''},\n",
       " {'context': 'depressiona nd mood are stable.. he is compliant with his cap.. gerd is stable.. robert has a past medical history of altered mental status ( 1 / 9 / 2016 ) ; bipolar disorder ( * ) ; cholelithiasis ; cirrhosis ( * ) ; cpap ( continuous positive airway pressure ) dependence ; depression ; diabetes mellitus ( * ) ; gerd ( gastroesophageal reflux disease ) ; hernia of unspecified site of abdominal cavity without mention of obstruction or gangrene ; ht',\n",
       "  'attribute': 'stage',\n",
       "  'true value': 'cirrhosis',\n",
       "  'predict value': 'cirrhosis'},\n",
       " {'context': 'i have reviewed the patients history and i have personally examined the patient.. i have reviewed all of the available laboratory and radiological studies.. i agree with the findings and plan of care documented in lucy mathews note, with any additions or modifications noted below.. in summary, this is a pleasant 48 - year - old female who was found to have fatty liver with no evidence of fibrosis.. she is also overweight.. advised patient to eat healthy and exercise and we will see her back in our clinic in 6 months time with labs prior to that.',\n",
       "  'attribute': 'stage',\n",
       "  'true value': 'no evidence of',\n",
       "  'predict value': 'no'}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d95955a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {0: tensor([  101, 12645,   172,  3161,  1197, 15342,  1548,  1104, 11911,  1443,\n",
       "                      1112, 14375,  1116,   119,  8362, 20080, 10294,  6202,  1119,  4163,\n",
       "                      2941,   172,  3161,  1197, 15342,  1548,  2076,   113,   177, 19515,\n",
       "                      3463,   114,   118,  2425,   119,  9468,  2737,   113,  1664,  1348,\n",
       "                      2528, 14084,  1596,   188,  1566, 10024,  4638,  4163, 27659,   114,\n",
       "                       102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0],\n",
       "                    device='cuda:0'),\n",
       "             1: tensor([  101,   113,   180, 26253,   119,  5615,   114,  1664,  1348,  2528,\n",
       "                     14084,  1596,   188,  1566, 10024,  4638,  4163, 27659,   119,   113,\n",
       "                       174,  1545,  1545,   119,   130,   114,   184, 27655,   117,   171,\n",
       "                      3080,  2588,   118,  3614,   119,   130,   117,  4457,   119,   182,\n",
       "                      1197,   119,   187,  6582,   175, 16664,  1468, 12666,  1110,   170,\n",
       "                      2724,   194,  1733,  2581,  1150,  8218,  1111, 10540,  1105,  2635,\n",
       "                      1104, 22026,   118,  2888, 13306,  1119,  4163, 27659,   171,  1105,\n",
       "                      9468,  2087,  5253,   120,  9468,  2737,   119,   119,  1119,  1144,\n",
       "                      3421,  1185, 13522,  1104, 13306, 11911,  3653,   117,  1216,  1112,\n",
       "                      1112, 14375,  1116,   117, 15661,  4396,  1348,  9793,   117,  1119,\n",
       "                      4163,  2941,  4035,  2093, 20695, 13200, 23610,   117, 20061, 19560,\n",
       "                      1679,  8383,  2605,  6620,   117,  1137,  1119,  4163,  2430, 18091,\n",
       "                      1610, 16430,  7903,   117,  1105,  1119,  1144,   102],\n",
       "                    device='cuda:0'),\n",
       "             2: tensor([  101,  7560,  1161,   183,  1181,  6601,  1132,  6111,   119,   119,\n",
       "                      1119,  1110,  3254, 27898,  1114,  1117,  6707,   119,   119,   176,\n",
       "                     25081,  1110,  6111,   119,   119, 11580,  3740,  1144,   170,  1763,\n",
       "                      2657,  1607,  1104,  8599,  4910,  2781,   113,   122,   120,   130,\n",
       "                       120,  1446,   114,   132, 16516, 23043,  1813,  8936,   113,   115,\n",
       "                       114,   132, 22572,  9016, 12888, 10652,  4863,   132,   172,  3161,\n",
       "                      1197, 15342,  1548,   113,   115,   114,   132,   172,  4163,  1643,\n",
       "                       113,  6803,  3112,  1586,  2787,  2997,   114, 20917,   132,  7560,\n",
       "                       132, 17972,  1143,  6473,  4814,   113,   115,   114,   132,   176,\n",
       "                     25081,   113,  3245,  8005,  1279,  4184, 19911,  1348,  1231,  2087,\n",
       "                     24796,  3653,   114,   132,  1123,  5813,  1104,  8362, 20080, 10294,\n",
       "                      6202,  1751,  1104, 24716, 19421,  1443,  4734,  1104,   184,  4832,\n",
       "                     17993,  1137,  6939, 16717,   132,   177,  1204,   102],\n",
       "                    device='cuda:0'),\n",
       "             3: tensor([  101,   178,  1138,  7815,  1103,  4420,  1607,  1105,   178,  1138,\n",
       "                      7572,  8600,  1103,  5351,   119,   119,   178,  1138,  7815,  1155,\n",
       "                      1104,  1103,  1907,  8087,  1105,  2070,  7810,  2527,   119,   119,\n",
       "                       178,  5340,  1114,  1103,  9505,  1105,  2197,  1104,  1920,  8510,\n",
       "                      1107,   181, 21977,  1183, 12523, 17540,  3805,   117,  1114,  1251,\n",
       "                     14101,  1137, 13334,  2382,  2071,   119,   119,  1107, 14940,   117,\n",
       "                      1142,  1110,   170, 10287,  3615,   118,  1214,   118,  1385,  2130,\n",
       "                      1150,  1108,  1276,  1106,  1138, 24862, 11911,  1114,  1185,  2554,\n",
       "                      1104, 20497, 12725,  4863,   119,   119,  1131,  1110,  1145,  1166,\n",
       "                      7150,   119,   119,  9213,  5351,  1106,  3940,  8071,  1105,  6730,\n",
       "                      1105,  1195,  1209,  1267,  1123,  1171,  1107,  1412, 12257,  1107,\n",
       "                       127,  1808,  1159,  1114, 21973,  2988,  1106,  1115,   119,   102,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0],\n",
       "                    device='cuda:0'),\n",
       "             4: tensor([  101,  1119,  1144,  1793,  1720,  1111,  1103,  8006,   119,   119,\n",
       "                      1103,  3252,  2136,  1185,  3893,   119,   119,  1119,  3756,  1119,\n",
       "                      3885,  1155,  1117,  2489,  1143,  3680,  1283,  1314,  2370,  1272,\n",
       "                      1119,  1464,  1119,  1108,   111,   186, 11848,  1204,   132, 15884,\n",
       "                       111,   186, 11848,  1204,   132,   119,   119, 11580,  3740,  1144,\n",
       "                       170,  1763,  2657,  1607,  1104,  8599,  4910,  2781,   113,   122,\n",
       "                       120,   130,   120,  1446,   114,   132, 16516, 23043,  1813,  8936,\n",
       "                       113,   115,   114,   132, 22572,  9016, 12888, 10652,  4863,   132,\n",
       "                       172,  3161,  1197, 15342,  1548,   113,   115,   114,   132,   172,\n",
       "                      4163,  1643,   113,  6803,  3112,  1586,  2787,  2997,   114, 20917,\n",
       "                       132,  7560,   132, 17972,  1143,  6473,  4814,   113,   115,   114,\n",
       "                       132,   176, 25081,   113,  3245,  8005,  1279,  4184, 19911,  1348,\n",
       "                      1231,  2087, 24796,  3653,   114,   132,  1123,   102],\n",
       "                    device='cuda:0'),\n",
       "             5: tensor([  101, 12645,   172,  3161,  1197, 15342,  1548,  1104, 11911,  1443,\n",
       "                      1112, 14375,  1116,   119,  8362, 20080, 10294,  6202,  1119,  4163,\n",
       "                      2941,   172,  3161,  1197, 15342,  1548,  2076,   113,   177, 19515,\n",
       "                      3463,   114,   119,  9468,  2737,   113,  1664,  1348,  2528, 14084,\n",
       "                      1596,   188,  1566, 10024,  4638,  4163, 27659,   114,   119,  1168,\n",
       "                     13306,  1664,  1348,  2528, 14084,  1596, 11911,  3653,   102,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0],\n",
       "                    device='cuda:0'),\n",
       "             6: tensor([  101,  8599,  4910,  2781,   122,   120,   130,   120,  1446,   119,\n",
       "                     16516, 23043,  1813,  8936,   113,   177, 19515,  3463,   114,   119,\n",
       "                     22572,  9016, 12888, 10652,  4863,   119,   172,  3161,  1197, 15342,\n",
       "                      1548,   113,   177, 19515,  3463,   114,   119,   172,  4163,  1643,\n",
       "                       113,  6803,  3112,  1586,  2787,  2997,   114, 20917,   119,  7560,\n",
       "                       102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0],\n",
       "                    device='cuda:0'),\n",
       "             7: tensor([  101,   130,   120,  1407,   120, 10351,  9468,  2737,   113,  1664,\n",
       "                      1348,  2528, 14084,  1596,   188,  1566, 10024,  4638,  4163, 27659,\n",
       "                       114,   119,  1168, 13306,  1664,  1348,  2528, 14084,  1596, 11911,\n",
       "                      3653,   119,   130,   120,  1407,   120, 10351,   172,  3161,  1197,\n",
       "                     15342,  1548,  1104, 11911,  1443,  1112, 14375,  1116,   119,  8362,\n",
       "                     20080, 10294,  6202,  1119,  4163,  2941,   172,  3161,  1197, 15342,\n",
       "                      1548,  2076,   113,   177, 19515,  3463,   114,   119,   130,   120,\n",
       "                      1627,   120, 10351,  9468,  2737,   113,  1664,  1348,  2528, 14084,\n",
       "                      1596,   188,  1566, 10024,  4638,  4163, 27659,   114,   119,  1168,\n",
       "                     13306,  1664,  1348,  2528, 14084,  1596, 11911,  3653,   102,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0],\n",
       "                    device='cuda:0'),\n",
       "             8: tensor([  101, 12645,  2236,   119, 16516, 23043,  1813,  8936,   113,   177,\n",
       "                     19515,   114,   119, 22572,  9016, 12888, 10652,  4863,   119,   172,\n",
       "                      3161,  1197, 15342,  1548,   113,   177, 19515,   114,   119,   172,\n",
       "                      4163,  1643,   113,  6803,  3112,  1586,  2787,  2997,   114, 20917,\n",
       "                       119,  7560,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0],\n",
       "                    device='cuda:0'),\n",
       "             9: tensor([  101,  8362, 20080, 10294,  6202,  1119,  4163,  2941,   172,  3161,\n",
       "                      1197, 15342,  1548,  2076,   113,   177, 19515,  3463,   114,   119,\n",
       "                       130,   120,   125,   120, 10351,  9468,  2737,   113,  1664,  1348,\n",
       "                      2528, 14084,  1596,   188,  1566, 10024,  4638,  4163, 27659,   114,\n",
       "                       119,  1168, 13306,  1664,  1348,  2528, 14084,  1596, 11911,  3653,\n",
       "                       119,   130,   120,   125,   120, 10351,   172,  3161,  1197, 15342,\n",
       "                      1548,  1104, 11911,  1443,  1112, 14375,  1116,   119,  8362, 20080,\n",
       "                     10294,  6202,  1119,  4163,  2941,   172,  3161,  1197, 15342,  1548,\n",
       "                      2076,   113,   177, 19515,  3463,   114,   119,   130,   120,  1407,\n",
       "                       120, 10351,  9468,  2737,   113,  1664,  1348,  2528, 14084,  1596,\n",
       "                       188,  1566, 10024,  4638,  4163, 27659,   114,   102,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0],\n",
       "                    device='cuda:0'),\n",
       "             10: tensor([  101,   171,  1643,  1105,   171,  1403,  3001,  1138,  1151,   183,\n",
       "                      1233,   119,   119,  6601,  1110,  6111,   119,   119,  1119,  5115,\n",
       "                      1218,  1114,  1185, 11344,  1137,  2492,   119,   119, 11580,  3740,\n",
       "                      1144,   170,  1763,  2657,  1607,  1104,  8599,  4910,  2781,   113,\n",
       "                       122,   120,   130,   120,  1446,   114,   117, 16516, 23043,  1813,\n",
       "                      8936,   113,   177, 19515,  3463,   114,   117, 22572,  9016, 12888,\n",
       "                     10652,  4863,   117,   172,  3161,  1197, 15342,  1548,   113,   177,\n",
       "                     19515,  3463,   114,   117,   172,  4163,  1643,   113,  6803,  3112,\n",
       "                      1586,  2787,  2997,   114, 20917,   117,  7560,   117, 17972,  1143,\n",
       "                      6473,  4814,   113,   177, 19515,  3463,   114,   117,   176, 25081,\n",
       "                       113,  3245,  8005,  1279,  4184, 19911,  1348,  1231,  2087, 24796,\n",
       "                      3653,   114,   117,  1123,  5813,  1104,  8362, 20080, 10294,  6202,\n",
       "                      1751,  1104, 24716, 19421,  1443,  4734,  1104,   102],\n",
       "                    device='cuda:0'),\n",
       "             11: tensor([  101,  5351,  2231,  1115,  1103,  4040,  2528,   126,   186,  1181,\n",
       "                       185,  4558, 24259,  1110,  1136,  9783,  1117,  2489,   119,   119,\n",
       "                      1119,  1156,  1176,  1106,  6265,  4138,  1103, 13753,  1106,  4040,\n",
       "                      2528,  1275,   118, 21380,  6875,   119,   119,  1119,  2231,  1119,\n",
       "                      1108,  1113,  1142, 13753,  1196,  1105,  1122,  4013,  1117,  1171,\n",
       "                      2489,   119,   119, 11580,  3740,  1144,   170,  1763,  2657,  1607,\n",
       "                      1104,  8599,  4910,  2781,   113,   122,   120,   130,   120,  1446,\n",
       "                       114,   117, 16516, 23043,  1813,  8936,   113,   177, 19515,  3463,\n",
       "                       114,   117, 22572,  9016, 12888, 10652,  4863,   117,   172,  3161,\n",
       "                      1197, 15342,  1548,   113,   177, 19515,  3463,   114,   117,   172,\n",
       "                      4163,  1643,   113,  6803,  3112,  1586,  2787,  2997,   114, 20917,\n",
       "                       117,  7560,   117, 17972,  1143,  6473,  4814,   113,   177, 19515,\n",
       "                      3463,   114,   117,   176, 25081,   113,  3245,   102],\n",
       "                    device='cuda:0'),\n",
       "             12: tensor([  101,  1103,  1954,  3252,  2790, 10496,  8331,  1104,  4764,  7540,\n",
       "                       119,   119, 14037,  2645,  1511,  8050, 21634,  1106, 10211,  1105,\n",
       "                      8050, 21634,  1106,  6730,   119,   119,  3187,  5320,  1111,  1884,\n",
       "                     15789,  1616, 18593,  3653,  1511, 17972,  1143,  6473,  4814,   117,\n",
       "                       173,  6834, 10913,  3269,  8191,   117,   177, 24312, 23826,  1988,\n",
       "                       117,  2581,  2673,  1105,   170, 14516, 11951,  3113,  9897,   119,\n",
       "                       119, 11580,  3740,  1144,   170,  1763,  2657,  1607,  1104,  8599,\n",
       "                      4910,  2781,   113,   122,   120,   130,   120,  1446,   114,   132,\n",
       "                     16516, 23043,  1813,  8936,   113,   177, 19515,  3463,   114,   132,\n",
       "                     22572,  9016, 12888, 10652,  4863,   132,   172,  3161,  1197, 15342,\n",
       "                      1548,   113,   177, 19515,  3463,   114,   132,   172,  4163,  1643,\n",
       "                       113,  6803,  3112,  1586,  2787,  2997,   114, 20917,   132,  7560,\n",
       "                       132, 17972,  1143,  6473,  4814,   113,   177,   102],\n",
       "                    device='cuda:0'),\n",
       "             13: tensor([  101, 12645,  9468,  2737,   113,  1664,  1348,  2528, 14084,  1596,\n",
       "                       188,  1566, 10024,  4638,  4163, 27659,   114,   119,  1168, 13306,\n",
       "                      1664,  1348,  2528, 14084,  1596, 11911,  3653,   119,   172,  3161,\n",
       "                      1197, 15342,  1548,  1104, 11911,  1443,  1112, 14375,  1116,   119,\n",
       "                      8362, 20080, 10294,  6202,  1119,  4163,  2941,   172,  3161,  1197,\n",
       "                     15342,  1548,  2076,   113,   177, 19515,  3463,   114,   119,  8510,\n",
       "                      1107,  1142,  8107,   102,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0],\n",
       "                    device='cuda:0'),\n",
       "             14: tensor([  101,  1185,  1954,  3695,   118,  8318, 23897,  1111,  1142,  3143,\n",
       "                       119,   119,  1763,  2657,  1607,   131,   119,   122,   119,   119,\n",
       "                     24862, 11911,  1359,  1113,   172,  1204, 14884,  1112,  1218,  1112,\n",
       "                       182,  1197,  8468, 12788,  9543,  1114,  1185,  2554,  1104, 20497,\n",
       "                     12725,  4863,   119,   119,   123,   119,   119,  2946,   170,  1643,\n",
       "                     25362,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0],\n",
       "                    device='cuda:0'),\n",
       "             15: tensor([  101,  1649,   117,  1175,  1110,  1185,  2554,  1104, 20497, 12725,\n",
       "                      4863,   117,  1134,  1110, 23310,   119,   119,  1195,  1328,  1106,\n",
       "                      3843,  1251, 20497, 12725,  4863,  1107,  1103,  2174,   119,   119,\n",
       "                      3335,   117,  1122,  1110,  1696,  1115,  1131,  2222,  1106,  3940,\n",
       "                      8071,  1105,  6730,  1105,  3857,  2841,   119,   119,  1195,  1209,\n",
       "                      1202,   182,  1874,  1451,   123,  1201,  1137,  1177,  1106,  8804,\n",
       "                     20497, 12725,  4863,   119,   119,  1195, 21744,  1115,  5351,  1144,\n",
       "                       185,  5114, 10242,  1106, 11872,   132,  3335,   117,  1123, 10211,\n",
       "                      1158,  1110,  4742,  2609,   119,   119,  1649,   117,  1195,  9213,\n",
       "                      5351,  1106,  2760,  1106,  2222,  1123,  1436,   119,   102,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                         0,     0,     0,     0,     0,     0,     0,     0],\n",
       "                    device='cuda:0')})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_context_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f90a344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_label_ids[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e9bf0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'diagnosis',\n",
       " 'c',\n",
       " '##ir',\n",
       " '##r',\n",
       " '##hos',\n",
       " '##is',\n",
       " 'of',\n",
       " 'liver',\n",
       " 'without',\n",
       " 'as',\n",
       " '##cite',\n",
       " '##s',\n",
       " '.',\n",
       " 'un',\n",
       " '##sp',\n",
       " '##ec',\n",
       " '##ified',\n",
       " 'he',\n",
       " '##pa',\n",
       " '##tic',\n",
       " 'c',\n",
       " '##ir',\n",
       " '##r',\n",
       " '##hos',\n",
       " '##is',\n",
       " 'type',\n",
       " '(',\n",
       " 'h',\n",
       " '##cc',\n",
       " 'code',\n",
       " ')',\n",
       " '-',\n",
       " 'primary',\n",
       " '.',\n",
       " 'na',\n",
       " '##sh',\n",
       " '(',\n",
       " 'non',\n",
       " '##al',\n",
       " '##co',\n",
       " '##hol',\n",
       " '##ic',\n",
       " 's',\n",
       " '##te',\n",
       " '##ato',\n",
       " '##he',\n",
       " '##pa',\n",
       " '##titis',\n",
       " ')',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(all_context_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c23eb06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq = 14\n",
    "predictions = all_predictions[qq]\n",
    "[tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(all_context_input_ids[qq][pred[1]: pred[2] + 1])) for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cb01a6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no evidence of']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq = 14\n",
    "labels = all_labels[qq]\n",
    "[tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(all_context_input_ids[qq][label[1]: label[2] + 1])) for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "00a973bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {0: [['a', 2, 6]],\n",
       "             1: [],\n",
       "             2: [['a', 58, 62]],\n",
       "             3: [['a', 78, 78]],\n",
       "             4: [['a', 80, 84]],\n",
       "             5: [['a', 2, 6]],\n",
       "             6: [['a', 26, 30]],\n",
       "             7: [['a', 37, 41]],\n",
       "             8: [['a', 19, 23]],\n",
       "             9: [['a', 56, 60]],\n",
       "             10: [['a', 63, 67]],\n",
       "             11: [['a', 88, 92]],\n",
       "             12: [['a', 96, 100]],\n",
       "             13: [['a', 28, 32]],\n",
       "             14: [['a', 36, 36]],\n",
       "             15: [['a', 5, 5]]})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2101f48a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {0: [['a', 2, 6]],\n",
       "             1: [],\n",
       "             2: [['a', 58, 62]],\n",
       "             3: [['a', 78, 80]],\n",
       "             4: [['a', 80, 84]],\n",
       "             5: [['a', 2, 6]],\n",
       "             6: [['a', 26, 30]],\n",
       "             7: [['a', 37, 41]],\n",
       "             8: [['a', 19, 23]],\n",
       "             9: [['a', 56, 60]],\n",
       "             10: [['a', 63, 67]],\n",
       "             11: [['a', 88, 92]],\n",
       "             12: [['a', 96, 100]],\n",
       "             13: [['a', 28, 32]],\n",
       "             14: [['a', 36, 38]],\n",
       "             15: [['a', 5, 11]]})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4718aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04505ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdad37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd0073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1376136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2983103d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c2fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23325fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fead11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "057f598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    kk=batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55d44bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "308d81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_output_context = bert_model(kk['context_input_ids'], kk['context_type_ids'], kk['context_input_mask'])\n",
    "bert_output_attribute = bert_model(kk['attribute_input_ids'], kk['attribute_type_ids'], kk['attribute_input_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2aefb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_lstm = nn.LSTM(input_size=config.hidden_size,\n",
    "                                    hidden_size=config.hidden_size // 2,\n",
    "                                    batch_first=True,\n",
    "                                    bidirectional=True)\n",
    "attribute_lstm = nn.LSTM(input_size=config.hidden_size,\n",
    "                              hidden_size=config.hidden_size // 2,\n",
    "                              batch_first=True,\n",
    "                              bidirectional=True)\n",
    "context_output, _ = context_lstm(bert_output_context[0])\n",
    "_, attribute_hidden = attribute_lstm(bert_output_attribute[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92390cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_output = torch.cat([attribute_hidden[0][-2], attribute_hidden[0][-1]], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fdc6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def forward(self, context_output, attribute_output):\n",
    "        seq_len = context_output.size()[1]\n",
    "        attribute_output = attribute_output.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        cos_sim = torch.cosine_similarity(context_output, attribute_output, -1)\n",
    "        cos_sim = cos_sim.unsqueeze(-1)\n",
    "        outputs = context_output * cos_sim\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61e6dabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 80, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = Attention()\n",
    "attention_output = attention(context_output, attribute_output)\n",
    "attention_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2eea21d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 80, 1536])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = torch.cat([context_output, attention_output], dim=-1)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "824ee933",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = LayerNorm(config.hidden_size * 2)\n",
    "dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "classifier = nn.Linear(config.hidden_size * 2, len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a985fed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 80, 1536])\n",
      "torch.Size([16, 80, 1536])\n",
      "torch.Size([16, 80, 5])\n"
     ]
    }
   ],
   "source": [
    "outputs = ln(outputs)\n",
    "print(outputs.shape)\n",
    "outputs = dropout(outputs)\n",
    "print(outputs.shape)\n",
    "logits = classifier(outputs)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2b0d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {k: i for i, k in enumerate(label_list)}\n",
    "\n",
    "crf = CRF(tagset_size=len(label_list), tag_dictionary=label2id, device=device, is_bert=True)\n",
    "loss = crf.calculate_loss(logits.to(device), tag_list=kk['label_ids'].to(device), lengths=kk['context_input_len'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6cf37b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20127.7031, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "1a2e5fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f737b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0ba862ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e2387f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(10, 20, num_layers=2)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d20bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3173dbf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4843a3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3cb6d708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6a3f77e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 80, 768])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_output_context[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2315c348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 768])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_output_context.pooler_output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cancer",
   "language": "python",
   "name": "cancer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
